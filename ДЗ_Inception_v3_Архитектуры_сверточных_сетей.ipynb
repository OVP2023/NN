{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOev1ZIHEORLis0cmFD0bE/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OVP2023/NN/blob/main/%D0%94%D0%97_Inception_v3_%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D1%8B_%D1%81%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D1%85_%D1%81%D0%B5%D1%82%D0%B5%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IhbIEYS1xuTl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision as tv\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6WxSsnzH17Lz",
        "outputId": "af267f66-deed-45d8-97a8-bca83eb3056a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0, 0\n",
        "    net.eval()\n",
        "    for X, y in data_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
        "        n += y.shape[0]\n",
        "    return acc_sum.item() / n\n",
        "\n",
        "def train(net, train_iter, test_iter, trainer, num_epochs):\n",
        "    net.to(device)\n",
        "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
        "    net.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            trainer.zero_grad()\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.shape[0]\n",
        "\n",
        "            if i % 10 == 0:\n",
        "              print(f\"Step {i}. time since epoch: {time.time() -  start:.3f}. \"\n",
        "                    f\"Train acc: {train_acc_sum / n:.3f}. Train Loss: {train_l_sum / n:.3f}\")\n",
        "        test_acc = evaluate_accuracy(test_iter, net.to(device))\n",
        "        print('-' * 20)\n",
        "        print(f'epoch {epoch + 1}, loss {train_l_sum / n:.4f}, train acc {train_acc_sum / n:.3f}'\n",
        "              f', test acc {test_acc:.3f}, time {time.time() - start:.1f} sec')"
      ],
      "metadata": {
        "id": "AhmpQeyrz60Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transoforms = tv.transforms.Compose([tv.transforms.Grayscale(3), tv.transforms.Resize((224, 224)), tv.transforms.ToTensor()])\n",
        "Batch_size = 64\n",
        "train_dataset = tv.datasets.EMNIST('.', split='balanced', train=True, download=True, transform=transoforms)\n",
        "test_dataset = tv.datasets.EMNIST('.', split='balanced', train=False, download=True, transform=transoforms)\n",
        "\n",
        "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=Batch_size, shuffle=True)\n",
        "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=Batch_size)"
      ],
      "metadata": {
        "id": "p7ctWT5dz68s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0acfc81-2201-4fb9-c007-7a44cabece5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 562M/562M [00:02<00:00, 192MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXvEuDt9-3pv",
        "outputId": "7758ab8e-e849-4481-b78d-519848afc74e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset EMNIST\n",
              "    Number of datapoints: 112800\n",
              "    Root location: .\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Grayscale(num_output_channels=3)\n",
              "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset.classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBsbFbYi_UK7",
        "outputId": "ebbe74e3-cc39-4f94-8a9d-3b7ea02e6e11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inception_v3 = tv.models.inception_v3(pretrained=False, aux_logits=False)\n",
        "model_inception_v3.fc = nn.Linear(in_features=2048, out_features=47)\n",
        "optimizer_inception_v3 = torch.optim.Adam(model_inception_v3.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks7aVH-ehxX4",
        "outputId": "c794dd6d-e89b-4190-9ac2-731032c3ff2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model_inception_v3.to(device), input_size=(3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_j2h4gw1B41",
        "outputId": "42b52585-0c7c-40d4-d2c8-f118de95c130"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 111, 111]             864\n",
            "       BatchNorm2d-2         [-1, 32, 111, 111]              64\n",
            "       BasicConv2d-3         [-1, 32, 111, 111]               0\n",
            "            Conv2d-4         [-1, 32, 109, 109]           9,216\n",
            "       BatchNorm2d-5         [-1, 32, 109, 109]              64\n",
            "       BasicConv2d-6         [-1, 32, 109, 109]               0\n",
            "            Conv2d-7         [-1, 64, 109, 109]          18,432\n",
            "       BatchNorm2d-8         [-1, 64, 109, 109]             128\n",
            "       BasicConv2d-9         [-1, 64, 109, 109]               0\n",
            "        MaxPool2d-10           [-1, 64, 54, 54]               0\n",
            "           Conv2d-11           [-1, 80, 54, 54]           5,120\n",
            "      BatchNorm2d-12           [-1, 80, 54, 54]             160\n",
            "      BasicConv2d-13           [-1, 80, 54, 54]               0\n",
            "           Conv2d-14          [-1, 192, 52, 52]         138,240\n",
            "      BatchNorm2d-15          [-1, 192, 52, 52]             384\n",
            "      BasicConv2d-16          [-1, 192, 52, 52]               0\n",
            "        MaxPool2d-17          [-1, 192, 25, 25]               0\n",
            "           Conv2d-18           [-1, 64, 25, 25]          12,288\n",
            "      BatchNorm2d-19           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-20           [-1, 64, 25, 25]               0\n",
            "           Conv2d-21           [-1, 48, 25, 25]           9,216\n",
            "      BatchNorm2d-22           [-1, 48, 25, 25]              96\n",
            "      BasicConv2d-23           [-1, 48, 25, 25]               0\n",
            "           Conv2d-24           [-1, 64, 25, 25]          76,800\n",
            "      BatchNorm2d-25           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-26           [-1, 64, 25, 25]               0\n",
            "           Conv2d-27           [-1, 64, 25, 25]          12,288\n",
            "      BatchNorm2d-28           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-29           [-1, 64, 25, 25]               0\n",
            "           Conv2d-30           [-1, 96, 25, 25]          55,296\n",
            "      BatchNorm2d-31           [-1, 96, 25, 25]             192\n",
            "      BasicConv2d-32           [-1, 96, 25, 25]               0\n",
            "           Conv2d-33           [-1, 96, 25, 25]          82,944\n",
            "      BatchNorm2d-34           [-1, 96, 25, 25]             192\n",
            "      BasicConv2d-35           [-1, 96, 25, 25]               0\n",
            "           Conv2d-36           [-1, 32, 25, 25]           6,144\n",
            "      BatchNorm2d-37           [-1, 32, 25, 25]              64\n",
            "      BasicConv2d-38           [-1, 32, 25, 25]               0\n",
            "       InceptionA-39          [-1, 256, 25, 25]               0\n",
            "           Conv2d-40           [-1, 64, 25, 25]          16,384\n",
            "      BatchNorm2d-41           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-42           [-1, 64, 25, 25]               0\n",
            "           Conv2d-43           [-1, 48, 25, 25]          12,288\n",
            "      BatchNorm2d-44           [-1, 48, 25, 25]              96\n",
            "      BasicConv2d-45           [-1, 48, 25, 25]               0\n",
            "           Conv2d-46           [-1, 64, 25, 25]          76,800\n",
            "      BatchNorm2d-47           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-48           [-1, 64, 25, 25]               0\n",
            "           Conv2d-49           [-1, 64, 25, 25]          16,384\n",
            "      BatchNorm2d-50           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-51           [-1, 64, 25, 25]               0\n",
            "           Conv2d-52           [-1, 96, 25, 25]          55,296\n",
            "      BatchNorm2d-53           [-1, 96, 25, 25]             192\n",
            "      BasicConv2d-54           [-1, 96, 25, 25]               0\n",
            "           Conv2d-55           [-1, 96, 25, 25]          82,944\n",
            "      BatchNorm2d-56           [-1, 96, 25, 25]             192\n",
            "      BasicConv2d-57           [-1, 96, 25, 25]               0\n",
            "           Conv2d-58           [-1, 64, 25, 25]          16,384\n",
            "      BatchNorm2d-59           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-60           [-1, 64, 25, 25]               0\n",
            "       InceptionA-61          [-1, 288, 25, 25]               0\n",
            "           Conv2d-62           [-1, 64, 25, 25]          18,432\n",
            "      BatchNorm2d-63           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-64           [-1, 64, 25, 25]               0\n",
            "           Conv2d-65           [-1, 48, 25, 25]          13,824\n",
            "      BatchNorm2d-66           [-1, 48, 25, 25]              96\n",
            "      BasicConv2d-67           [-1, 48, 25, 25]               0\n",
            "           Conv2d-68           [-1, 64, 25, 25]          76,800\n",
            "      BatchNorm2d-69           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-70           [-1, 64, 25, 25]               0\n",
            "           Conv2d-71           [-1, 64, 25, 25]          18,432\n",
            "      BatchNorm2d-72           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-73           [-1, 64, 25, 25]               0\n",
            "           Conv2d-74           [-1, 96, 25, 25]          55,296\n",
            "      BatchNorm2d-75           [-1, 96, 25, 25]             192\n",
            "      BasicConv2d-76           [-1, 96, 25, 25]               0\n",
            "           Conv2d-77           [-1, 96, 25, 25]          82,944\n",
            "      BatchNorm2d-78           [-1, 96, 25, 25]             192\n",
            "      BasicConv2d-79           [-1, 96, 25, 25]               0\n",
            "           Conv2d-80           [-1, 64, 25, 25]          18,432\n",
            "      BatchNorm2d-81           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-82           [-1, 64, 25, 25]               0\n",
            "       InceptionA-83          [-1, 288, 25, 25]               0\n",
            "           Conv2d-84          [-1, 384, 12, 12]         995,328\n",
            "      BatchNorm2d-85          [-1, 384, 12, 12]             768\n",
            "      BasicConv2d-86          [-1, 384, 12, 12]               0\n",
            "           Conv2d-87           [-1, 64, 25, 25]          18,432\n",
            "      BatchNorm2d-88           [-1, 64, 25, 25]             128\n",
            "      BasicConv2d-89           [-1, 64, 25, 25]               0\n",
            "           Conv2d-90           [-1, 96, 25, 25]          55,296\n",
            "      BatchNorm2d-91           [-1, 96, 25, 25]             192\n",
            "      BasicConv2d-92           [-1, 96, 25, 25]               0\n",
            "           Conv2d-93           [-1, 96, 12, 12]          82,944\n",
            "      BatchNorm2d-94           [-1, 96, 12, 12]             192\n",
            "      BasicConv2d-95           [-1, 96, 12, 12]               0\n",
            "       InceptionB-96          [-1, 768, 12, 12]               0\n",
            "           Conv2d-97          [-1, 192, 12, 12]         147,456\n",
            "      BatchNorm2d-98          [-1, 192, 12, 12]             384\n",
            "      BasicConv2d-99          [-1, 192, 12, 12]               0\n",
            "          Conv2d-100          [-1, 128, 12, 12]          98,304\n",
            "     BatchNorm2d-101          [-1, 128, 12, 12]             256\n",
            "     BasicConv2d-102          [-1, 128, 12, 12]               0\n",
            "          Conv2d-103          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-104          [-1, 128, 12, 12]             256\n",
            "     BasicConv2d-105          [-1, 128, 12, 12]               0\n",
            "          Conv2d-106          [-1, 192, 12, 12]         172,032\n",
            "     BatchNorm2d-107          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-108          [-1, 192, 12, 12]               0\n",
            "          Conv2d-109          [-1, 128, 12, 12]          98,304\n",
            "     BatchNorm2d-110          [-1, 128, 12, 12]             256\n",
            "     BasicConv2d-111          [-1, 128, 12, 12]               0\n",
            "          Conv2d-112          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-113          [-1, 128, 12, 12]             256\n",
            "     BasicConv2d-114          [-1, 128, 12, 12]               0\n",
            "          Conv2d-115          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-116          [-1, 128, 12, 12]             256\n",
            "     BasicConv2d-117          [-1, 128, 12, 12]               0\n",
            "          Conv2d-118          [-1, 128, 12, 12]         114,688\n",
            "     BatchNorm2d-119          [-1, 128, 12, 12]             256\n",
            "     BasicConv2d-120          [-1, 128, 12, 12]               0\n",
            "          Conv2d-121          [-1, 192, 12, 12]         172,032\n",
            "     BatchNorm2d-122          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-123          [-1, 192, 12, 12]               0\n",
            "          Conv2d-124          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-125          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-126          [-1, 192, 12, 12]               0\n",
            "      InceptionC-127          [-1, 768, 12, 12]               0\n",
            "          Conv2d-128          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-129          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-130          [-1, 192, 12, 12]               0\n",
            "          Conv2d-131          [-1, 160, 12, 12]         122,880\n",
            "     BatchNorm2d-132          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-133          [-1, 160, 12, 12]               0\n",
            "          Conv2d-134          [-1, 160, 12, 12]         179,200\n",
            "     BatchNorm2d-135          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-136          [-1, 160, 12, 12]               0\n",
            "          Conv2d-137          [-1, 192, 12, 12]         215,040\n",
            "     BatchNorm2d-138          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-139          [-1, 192, 12, 12]               0\n",
            "          Conv2d-140          [-1, 160, 12, 12]         122,880\n",
            "     BatchNorm2d-141          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-142          [-1, 160, 12, 12]               0\n",
            "          Conv2d-143          [-1, 160, 12, 12]         179,200\n",
            "     BatchNorm2d-144          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-145          [-1, 160, 12, 12]               0\n",
            "          Conv2d-146          [-1, 160, 12, 12]         179,200\n",
            "     BatchNorm2d-147          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-148          [-1, 160, 12, 12]               0\n",
            "          Conv2d-149          [-1, 160, 12, 12]         179,200\n",
            "     BatchNorm2d-150          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-151          [-1, 160, 12, 12]               0\n",
            "          Conv2d-152          [-1, 192, 12, 12]         215,040\n",
            "     BatchNorm2d-153          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-154          [-1, 192, 12, 12]               0\n",
            "          Conv2d-155          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-156          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-157          [-1, 192, 12, 12]               0\n",
            "      InceptionC-158          [-1, 768, 12, 12]               0\n",
            "          Conv2d-159          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-160          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-161          [-1, 192, 12, 12]               0\n",
            "          Conv2d-162          [-1, 160, 12, 12]         122,880\n",
            "     BatchNorm2d-163          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-164          [-1, 160, 12, 12]               0\n",
            "          Conv2d-165          [-1, 160, 12, 12]         179,200\n",
            "     BatchNorm2d-166          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-167          [-1, 160, 12, 12]               0\n",
            "          Conv2d-168          [-1, 192, 12, 12]         215,040\n",
            "     BatchNorm2d-169          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-170          [-1, 192, 12, 12]               0\n",
            "          Conv2d-171          [-1, 160, 12, 12]         122,880\n",
            "     BatchNorm2d-172          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-173          [-1, 160, 12, 12]               0\n",
            "          Conv2d-174          [-1, 160, 12, 12]         179,200\n",
            "     BatchNorm2d-175          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-176          [-1, 160, 12, 12]               0\n",
            "          Conv2d-177          [-1, 160, 12, 12]         179,200\n",
            "     BatchNorm2d-178          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-179          [-1, 160, 12, 12]               0\n",
            "          Conv2d-180          [-1, 160, 12, 12]         179,200\n",
            "     BatchNorm2d-181          [-1, 160, 12, 12]             320\n",
            "     BasicConv2d-182          [-1, 160, 12, 12]               0\n",
            "          Conv2d-183          [-1, 192, 12, 12]         215,040\n",
            "     BatchNorm2d-184          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-185          [-1, 192, 12, 12]               0\n",
            "          Conv2d-186          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-187          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-188          [-1, 192, 12, 12]               0\n",
            "      InceptionC-189          [-1, 768, 12, 12]               0\n",
            "          Conv2d-190          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-191          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-192          [-1, 192, 12, 12]               0\n",
            "          Conv2d-193          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-194          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-195          [-1, 192, 12, 12]               0\n",
            "          Conv2d-196          [-1, 192, 12, 12]         258,048\n",
            "     BatchNorm2d-197          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-198          [-1, 192, 12, 12]               0\n",
            "          Conv2d-199          [-1, 192, 12, 12]         258,048\n",
            "     BatchNorm2d-200          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-201          [-1, 192, 12, 12]               0\n",
            "          Conv2d-202          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-203          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-204          [-1, 192, 12, 12]               0\n",
            "          Conv2d-205          [-1, 192, 12, 12]         258,048\n",
            "     BatchNorm2d-206          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-207          [-1, 192, 12, 12]               0\n",
            "          Conv2d-208          [-1, 192, 12, 12]         258,048\n",
            "     BatchNorm2d-209          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-210          [-1, 192, 12, 12]               0\n",
            "          Conv2d-211          [-1, 192, 12, 12]         258,048\n",
            "     BatchNorm2d-212          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-213          [-1, 192, 12, 12]               0\n",
            "          Conv2d-214          [-1, 192, 12, 12]         258,048\n",
            "     BatchNorm2d-215          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-216          [-1, 192, 12, 12]               0\n",
            "          Conv2d-217          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-218          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-219          [-1, 192, 12, 12]               0\n",
            "      InceptionC-220          [-1, 768, 12, 12]               0\n",
            "          Conv2d-221          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-222          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-223          [-1, 192, 12, 12]               0\n",
            "          Conv2d-224            [-1, 320, 5, 5]         552,960\n",
            "     BatchNorm2d-225            [-1, 320, 5, 5]             640\n",
            "     BasicConv2d-226            [-1, 320, 5, 5]               0\n",
            "          Conv2d-227          [-1, 192, 12, 12]         147,456\n",
            "     BatchNorm2d-228          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-229          [-1, 192, 12, 12]               0\n",
            "          Conv2d-230          [-1, 192, 12, 12]         258,048\n",
            "     BatchNorm2d-231          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-232          [-1, 192, 12, 12]               0\n",
            "          Conv2d-233          [-1, 192, 12, 12]         258,048\n",
            "     BatchNorm2d-234          [-1, 192, 12, 12]             384\n",
            "     BasicConv2d-235          [-1, 192, 12, 12]               0\n",
            "          Conv2d-236            [-1, 192, 5, 5]         331,776\n",
            "     BatchNorm2d-237            [-1, 192, 5, 5]             384\n",
            "     BasicConv2d-238            [-1, 192, 5, 5]               0\n",
            "      InceptionD-239           [-1, 1280, 5, 5]               0\n",
            "          Conv2d-240            [-1, 320, 5, 5]         409,600\n",
            "     BatchNorm2d-241            [-1, 320, 5, 5]             640\n",
            "     BasicConv2d-242            [-1, 320, 5, 5]               0\n",
            "          Conv2d-243            [-1, 384, 5, 5]         491,520\n",
            "     BatchNorm2d-244            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-245            [-1, 384, 5, 5]               0\n",
            "          Conv2d-246            [-1, 384, 5, 5]         442,368\n",
            "     BatchNorm2d-247            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-248            [-1, 384, 5, 5]               0\n",
            "          Conv2d-249            [-1, 384, 5, 5]         442,368\n",
            "     BatchNorm2d-250            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-251            [-1, 384, 5, 5]               0\n",
            "          Conv2d-252            [-1, 448, 5, 5]         573,440\n",
            "     BatchNorm2d-253            [-1, 448, 5, 5]             896\n",
            "     BasicConv2d-254            [-1, 448, 5, 5]               0\n",
            "          Conv2d-255            [-1, 384, 5, 5]       1,548,288\n",
            "     BatchNorm2d-256            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-257            [-1, 384, 5, 5]               0\n",
            "          Conv2d-258            [-1, 384, 5, 5]         442,368\n",
            "     BatchNorm2d-259            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-260            [-1, 384, 5, 5]               0\n",
            "          Conv2d-261            [-1, 384, 5, 5]         442,368\n",
            "     BatchNorm2d-262            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-263            [-1, 384, 5, 5]               0\n",
            "          Conv2d-264            [-1, 192, 5, 5]         245,760\n",
            "     BatchNorm2d-265            [-1, 192, 5, 5]             384\n",
            "     BasicConv2d-266            [-1, 192, 5, 5]               0\n",
            "      InceptionE-267           [-1, 2048, 5, 5]               0\n",
            "          Conv2d-268            [-1, 320, 5, 5]         655,360\n",
            "     BatchNorm2d-269            [-1, 320, 5, 5]             640\n",
            "     BasicConv2d-270            [-1, 320, 5, 5]               0\n",
            "          Conv2d-271            [-1, 384, 5, 5]         786,432\n",
            "     BatchNorm2d-272            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-273            [-1, 384, 5, 5]               0\n",
            "          Conv2d-274            [-1, 384, 5, 5]         442,368\n",
            "     BatchNorm2d-275            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-276            [-1, 384, 5, 5]               0\n",
            "          Conv2d-277            [-1, 384, 5, 5]         442,368\n",
            "     BatchNorm2d-278            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-279            [-1, 384, 5, 5]               0\n",
            "          Conv2d-280            [-1, 448, 5, 5]         917,504\n",
            "     BatchNorm2d-281            [-1, 448, 5, 5]             896\n",
            "     BasicConv2d-282            [-1, 448, 5, 5]               0\n",
            "          Conv2d-283            [-1, 384, 5, 5]       1,548,288\n",
            "     BatchNorm2d-284            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-285            [-1, 384, 5, 5]               0\n",
            "          Conv2d-286            [-1, 384, 5, 5]         442,368\n",
            "     BatchNorm2d-287            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-288            [-1, 384, 5, 5]               0\n",
            "          Conv2d-289            [-1, 384, 5, 5]         442,368\n",
            "     BatchNorm2d-290            [-1, 384, 5, 5]             768\n",
            "     BasicConv2d-291            [-1, 384, 5, 5]               0\n",
            "          Conv2d-292            [-1, 192, 5, 5]         393,216\n",
            "     BatchNorm2d-293            [-1, 192, 5, 5]             384\n",
            "     BasicConv2d-294            [-1, 192, 5, 5]               0\n",
            "      InceptionE-295           [-1, 2048, 5, 5]               0\n",
            "AdaptiveAvgPool2d-296           [-1, 2048, 1, 1]               0\n",
            "         Dropout-297           [-1, 2048, 1, 1]               0\n",
            "          Linear-298                   [-1, 47]          96,303\n",
            "================================================================\n",
            "Total params: 21,881,871\n",
            "Trainable params: 21,881,871\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 117.97\n",
            "Params size (MB): 83.47\n",
            "Estimated Total Size (MB): 202.02\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train(model_inception_v3, train_data, test_data, optimizer_inception_v3,1)"
      ],
      "metadata": {
        "id": "UQhq4UqgKtHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c9879d-9955-4c60-a855-734845b61681"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0. time since epoch: 1.203. Train acc: 0.031. Train Loss: 3.934\n",
            "Step 10. time since epoch: 6.709. Train acc: 0.051. Train Loss: 3.928\n",
            "Step 20. time since epoch: 12.041. Train acc: 0.083. Train Loss: 3.734\n",
            "Step 30. time since epoch: 17.540. Train acc: 0.121. Train Loss: 3.489\n",
            "Step 40. time since epoch: 22.913. Train acc: 0.180. Train Loss: 3.174\n",
            "Step 50. time since epoch: 28.414. Train acc: 0.248. Train Loss: 2.882\n",
            "Step 60. time since epoch: 33.832. Train acc: 0.302. Train Loss: 2.642\n",
            "Step 70. time since epoch: 40.089. Train acc: 0.352. Train Loss: 2.433\n",
            "Step 80. time since epoch: 45.570. Train acc: 0.387. Train Loss: 2.282\n",
            "Step 90. time since epoch: 51.122. Train acc: 0.418. Train Loss: 2.140\n",
            "Step 100. time since epoch: 56.736. Train acc: 0.448. Train Loss: 2.018\n",
            "Step 110. time since epoch: 62.324. Train acc: 0.473. Train Loss: 1.909\n",
            "Step 120. time since epoch: 68.099. Train acc: 0.498. Train Loss: 1.807\n",
            "Step 130. time since epoch: 73.760. Train acc: 0.517. Train Loss: 1.721\n",
            "Step 140. time since epoch: 79.517. Train acc: 0.533. Train Loss: 1.657\n",
            "Step 150. time since epoch: 85.095. Train acc: 0.546. Train Loss: 1.596\n",
            "Step 160. time since epoch: 90.756. Train acc: 0.558. Train Loss: 1.543\n",
            "Step 170. time since epoch: 96.280. Train acc: 0.571. Train Loss: 1.490\n",
            "Step 180. time since epoch: 101.906. Train acc: 0.582. Train Loss: 1.447\n",
            "Step 190. time since epoch: 107.381. Train acc: 0.592. Train Loss: 1.409\n",
            "Step 200. time since epoch: 113.022. Train acc: 0.603. Train Loss: 1.368\n",
            "Step 210. time since epoch: 118.481. Train acc: 0.611. Train Loss: 1.336\n",
            "Step 220. time since epoch: 124.039. Train acc: 0.617. Train Loss: 1.309\n",
            "Step 230. time since epoch: 129.583. Train acc: 0.624. Train Loss: 1.278\n",
            "Step 240. time since epoch: 135.076. Train acc: 0.632. Train Loss: 1.251\n",
            "Step 250. time since epoch: 140.708. Train acc: 0.637. Train Loss: 1.227\n",
            "Step 260. time since epoch: 146.251. Train acc: 0.643. Train Loss: 1.203\n",
            "Step 270. time since epoch: 151.956. Train acc: 0.648. Train Loss: 1.180\n",
            "Step 280. time since epoch: 157.514. Train acc: 0.653. Train Loss: 1.163\n",
            "Step 290. time since epoch: 163.166. Train acc: 0.658. Train Loss: 1.146\n",
            "Step 300. time since epoch: 168.716. Train acc: 0.662. Train Loss: 1.128\n",
            "Step 310. time since epoch: 174.416. Train acc: 0.667. Train Loss: 1.111\n",
            "Step 320. time since epoch: 179.945. Train acc: 0.671. Train Loss: 1.096\n",
            "Step 330. time since epoch: 185.571. Train acc: 0.675. Train Loss: 1.079\n",
            "Step 340. time since epoch: 191.080. Train acc: 0.680. Train Loss: 1.064\n",
            "Step 350. time since epoch: 196.692. Train acc: 0.684. Train Loss: 1.051\n",
            "Step 360. time since epoch: 202.239. Train acc: 0.687. Train Loss: 1.038\n",
            "Step 370. time since epoch: 207.787. Train acc: 0.690. Train Loss: 1.025\n",
            "Step 380. time since epoch: 213.356. Train acc: 0.693. Train Loss: 1.013\n",
            "Step 390. time since epoch: 218.869. Train acc: 0.696. Train Loss: 1.002\n",
            "Step 400. time since epoch: 224.499. Train acc: 0.699. Train Loss: 0.992\n",
            "Step 410. time since epoch: 230.010. Train acc: 0.701. Train Loss: 0.981\n",
            "Step 420. time since epoch: 235.671. Train acc: 0.704. Train Loss: 0.970\n",
            "Step 430. time since epoch: 241.211. Train acc: 0.707. Train Loss: 0.960\n",
            "Step 440. time since epoch: 246.865. Train acc: 0.709. Train Loss: 0.951\n",
            "Step 450. time since epoch: 252.386. Train acc: 0.711. Train Loss: 0.943\n",
            "Step 460. time since epoch: 258.073. Train acc: 0.714. Train Loss: 0.934\n",
            "Step 470. time since epoch: 263.577. Train acc: 0.716. Train Loss: 0.924\n",
            "Step 480. time since epoch: 269.202. Train acc: 0.719. Train Loss: 0.916\n",
            "Step 490. time since epoch: 274.694. Train acc: 0.721. Train Loss: 0.908\n",
            "Step 500. time since epoch: 280.260. Train acc: 0.722. Train Loss: 0.901\n",
            "Step 510. time since epoch: 285.846. Train acc: 0.724. Train Loss: 0.894\n",
            "Step 520. time since epoch: 291.350. Train acc: 0.726. Train Loss: 0.886\n",
            "Step 530. time since epoch: 297.006. Train acc: 0.728. Train Loss: 0.879\n",
            "Step 540. time since epoch: 302.507. Train acc: 0.730. Train Loss: 0.872\n",
            "Step 550. time since epoch: 308.151. Train acc: 0.732. Train Loss: 0.865\n",
            "Step 560. time since epoch: 313.673. Train acc: 0.733. Train Loss: 0.859\n",
            "Step 570. time since epoch: 319.354. Train acc: 0.735. Train Loss: 0.853\n",
            "Step 580. time since epoch: 324.854. Train acc: 0.736. Train Loss: 0.846\n",
            "Step 590. time since epoch: 330.484. Train acc: 0.738. Train Loss: 0.840\n",
            "Step 600. time since epoch: 336.010. Train acc: 0.739. Train Loss: 0.836\n",
            "Step 610. time since epoch: 341.690. Train acc: 0.741. Train Loss: 0.830\n",
            "Step 620. time since epoch: 347.205. Train acc: 0.742. Train Loss: 0.826\n",
            "Step 630. time since epoch: 352.793. Train acc: 0.743. Train Loss: 0.822\n",
            "Step 640. time since epoch: 358.340. Train acc: 0.744. Train Loss: 0.817\n",
            "Step 650. time since epoch: 363.855. Train acc: 0.745. Train Loss: 0.812\n",
            "Step 660. time since epoch: 369.482. Train acc: 0.747. Train Loss: 0.806\n",
            "Step 670. time since epoch: 375.004. Train acc: 0.748. Train Loss: 0.802\n",
            "Step 680. time since epoch: 380.662. Train acc: 0.750. Train Loss: 0.796\n",
            "Step 690. time since epoch: 386.182. Train acc: 0.751. Train Loss: 0.792\n",
            "Step 700. time since epoch: 391.788. Train acc: 0.752. Train Loss: 0.786\n",
            "Step 710. time since epoch: 397.328. Train acc: 0.753. Train Loss: 0.782\n",
            "Step 720. time since epoch: 403.002. Train acc: 0.754. Train Loss: 0.778\n",
            "Step 730. time since epoch: 408.541. Train acc: 0.755. Train Loss: 0.775\n",
            "Step 740. time since epoch: 414.169. Train acc: 0.756. Train Loss: 0.771\n",
            "Step 750. time since epoch: 419.665. Train acc: 0.757. Train Loss: 0.767\n",
            "Step 760. time since epoch: 425.324. Train acc: 0.758. Train Loss: 0.763\n",
            "Step 770. time since epoch: 430.875. Train acc: 0.759. Train Loss: 0.760\n",
            "Step 780. time since epoch: 436.425. Train acc: 0.760. Train Loss: 0.758\n",
            "Step 790. time since epoch: 442.001. Train acc: 0.761. Train Loss: 0.754\n",
            "Step 800. time since epoch: 447.516. Train acc: 0.762. Train Loss: 0.751\n",
            "Step 810. time since epoch: 453.186. Train acc: 0.763. Train Loss: 0.748\n",
            "Step 820. time since epoch: 458.710. Train acc: 0.764. Train Loss: 0.745\n",
            "Step 830. time since epoch: 464.333. Train acc: 0.765. Train Loss: 0.741\n",
            "Step 840. time since epoch: 469.850. Train acc: 0.766. Train Loss: 0.738\n",
            "Step 850. time since epoch: 475.473. Train acc: 0.767. Train Loss: 0.735\n",
            "Step 860. time since epoch: 480.996. Train acc: 0.768. Train Loss: 0.732\n",
            "Step 870. time since epoch: 486.663. Train acc: 0.769. Train Loss: 0.728\n",
            "Step 880. time since epoch: 492.185. Train acc: 0.769. Train Loss: 0.724\n",
            "Step 890. time since epoch: 497.829. Train acc: 0.770. Train Loss: 0.721\n",
            "Step 900. time since epoch: 503.360. Train acc: 0.771. Train Loss: 0.718\n",
            "Step 910. time since epoch: 508.953. Train acc: 0.772. Train Loss: 0.715\n",
            "Step 920. time since epoch: 514.561. Train acc: 0.772. Train Loss: 0.712\n",
            "Step 930. time since epoch: 520.090. Train acc: 0.773. Train Loss: 0.709\n",
            "Step 940. time since epoch: 525.709. Train acc: 0.774. Train Loss: 0.706\n",
            "Step 950. time since epoch: 531.237. Train acc: 0.775. Train Loss: 0.703\n",
            "Step 960. time since epoch: 536.903. Train acc: 0.776. Train Loss: 0.701\n",
            "Step 970. time since epoch: 542.437. Train acc: 0.777. Train Loss: 0.698\n",
            "Step 980. time since epoch: 548.126. Train acc: 0.777. Train Loss: 0.696\n",
            "Step 990. time since epoch: 553.650. Train acc: 0.778. Train Loss: 0.694\n",
            "Step 1000. time since epoch: 559.313. Train acc: 0.778. Train Loss: 0.692\n",
            "Step 1010. time since epoch: 564.807. Train acc: 0.779. Train Loss: 0.689\n",
            "Step 1020. time since epoch: 570.433. Train acc: 0.779. Train Loss: 0.686\n",
            "Step 1030. time since epoch: 575.952. Train acc: 0.780. Train Loss: 0.684\n",
            "Step 1040. time since epoch: 581.557. Train acc: 0.781. Train Loss: 0.682\n",
            "Step 1050. time since epoch: 587.105. Train acc: 0.782. Train Loss: 0.679\n",
            "Step 1060. time since epoch: 592.673. Train acc: 0.782. Train Loss: 0.678\n",
            "Step 1070. time since epoch: 598.248. Train acc: 0.783. Train Loss: 0.676\n",
            "Step 1080. time since epoch: 603.757. Train acc: 0.783. Train Loss: 0.674\n",
            "Step 1090. time since epoch: 609.424. Train acc: 0.784. Train Loss: 0.673\n",
            "Step 1100. time since epoch: 614.937. Train acc: 0.784. Train Loss: 0.671\n",
            "Step 1110. time since epoch: 620.562. Train acc: 0.784. Train Loss: 0.669\n",
            "Step 1120. time since epoch: 626.092. Train acc: 0.785. Train Loss: 0.666\n",
            "Step 1130. time since epoch: 631.774. Train acc: 0.786. Train Loss: 0.665\n",
            "Step 1140. time since epoch: 637.312. Train acc: 0.786. Train Loss: 0.663\n",
            "Step 1150. time since epoch: 642.933. Train acc: 0.787. Train Loss: 0.660\n",
            "Step 1160. time since epoch: 648.451. Train acc: 0.787. Train Loss: 0.658\n",
            "Step 1170. time since epoch: 654.069. Train acc: 0.788. Train Loss: 0.656\n",
            "Step 1180. time since epoch: 659.556. Train acc: 0.788. Train Loss: 0.655\n",
            "Step 1190. time since epoch: 665.135. Train acc: 0.789. Train Loss: 0.653\n",
            "Step 1200. time since epoch: 670.698. Train acc: 0.789. Train Loss: 0.651\n",
            "Step 1210. time since epoch: 676.199. Train acc: 0.790. Train Loss: 0.649\n",
            "Step 1220. time since epoch: 681.804. Train acc: 0.790. Train Loss: 0.648\n",
            "Step 1230. time since epoch: 687.305. Train acc: 0.791. Train Loss: 0.646\n",
            "Step 1240. time since epoch: 692.950. Train acc: 0.791. Train Loss: 0.645\n",
            "Step 1250. time since epoch: 698.463. Train acc: 0.791. Train Loss: 0.643\n",
            "Step 1260. time since epoch: 704.101. Train acc: 0.792. Train Loss: 0.641\n",
            "Step 1270. time since epoch: 709.629. Train acc: 0.792. Train Loss: 0.639\n",
            "Step 1280. time since epoch: 715.268. Train acc: 0.793. Train Loss: 0.638\n",
            "Step 1290. time since epoch: 720.777. Train acc: 0.793. Train Loss: 0.636\n",
            "Step 1300. time since epoch: 726.481. Train acc: 0.794. Train Loss: 0.635\n",
            "Step 1310. time since epoch: 732.018. Train acc: 0.794. Train Loss: 0.634\n",
            "Step 1320. time since epoch: 737.607. Train acc: 0.794. Train Loss: 0.633\n",
            "Step 1330. time since epoch: 743.148. Train acc: 0.795. Train Loss: 0.631\n",
            "Step 1340. time since epoch: 748.702. Train acc: 0.795. Train Loss: 0.629\n",
            "Step 1350. time since epoch: 754.323. Train acc: 0.795. Train Loss: 0.628\n",
            "Step 1360. time since epoch: 759.837. Train acc: 0.796. Train Loss: 0.626\n",
            "Step 1370. time since epoch: 765.471. Train acc: 0.796. Train Loss: 0.625\n",
            "Step 1380. time since epoch: 770.972. Train acc: 0.797. Train Loss: 0.624\n",
            "Step 1390. time since epoch: 776.613. Train acc: 0.797. Train Loss: 0.622\n",
            "Step 1400. time since epoch: 782.137. Train acc: 0.797. Train Loss: 0.621\n",
            "Step 1410. time since epoch: 787.817. Train acc: 0.798. Train Loss: 0.620\n",
            "Step 1420. time since epoch: 793.319. Train acc: 0.798. Train Loss: 0.619\n",
            "Step 1430. time since epoch: 798.961. Train acc: 0.798. Train Loss: 0.617\n",
            "Step 1440. time since epoch: 804.474. Train acc: 0.799. Train Loss: 0.616\n",
            "Step 1450. time since epoch: 810.116. Train acc: 0.799. Train Loss: 0.614\n",
            "Step 1460. time since epoch: 815.643. Train acc: 0.800. Train Loss: 0.613\n",
            "Step 1470. time since epoch: 821.202. Train acc: 0.800. Train Loss: 0.611\n",
            "Step 1480. time since epoch: 826.792. Train acc: 0.800. Train Loss: 0.610\n",
            "Step 1490. time since epoch: 832.297. Train acc: 0.801. Train Loss: 0.609\n",
            "Step 1500. time since epoch: 837.980. Train acc: 0.801. Train Loss: 0.607\n",
            "Step 1510. time since epoch: 843.498. Train acc: 0.802. Train Loss: 0.606\n",
            "Step 1520. time since epoch: 849.159. Train acc: 0.802. Train Loss: 0.605\n",
            "Step 1530. time since epoch: 854.661. Train acc: 0.802. Train Loss: 0.604\n",
            "Step 1540. time since epoch: 860.308. Train acc: 0.802. Train Loss: 0.603\n",
            "Step 1550. time since epoch: 865.832. Train acc: 0.803. Train Loss: 0.602\n",
            "Step 1560. time since epoch: 871.460. Train acc: 0.803. Train Loss: 0.601\n",
            "Step 1570. time since epoch: 876.980. Train acc: 0.803. Train Loss: 0.600\n",
            "Step 1580. time since epoch: 882.676. Train acc: 0.804. Train Loss: 0.599\n",
            "Step 1590. time since epoch: 888.198. Train acc: 0.804. Train Loss: 0.598\n",
            "Step 1600. time since epoch: 893.790. Train acc: 0.804. Train Loss: 0.597\n",
            "Step 1610. time since epoch: 899.342. Train acc: 0.805. Train Loss: 0.596\n",
            "Step 1620. time since epoch: 904.875. Train acc: 0.805. Train Loss: 0.595\n",
            "Step 1630. time since epoch: 910.494. Train acc: 0.805. Train Loss: 0.594\n",
            "Step 1640. time since epoch: 916.027. Train acc: 0.805. Train Loss: 0.593\n",
            "Step 1650. time since epoch: 921.670. Train acc: 0.806. Train Loss: 0.592\n",
            "Step 1660. time since epoch: 927.154. Train acc: 0.806. Train Loss: 0.591\n",
            "Step 1670. time since epoch: 932.781. Train acc: 0.806. Train Loss: 0.590\n",
            "Step 1680. time since epoch: 938.300. Train acc: 0.807. Train Loss: 0.589\n",
            "Step 1690. time since epoch: 943.965. Train acc: 0.807. Train Loss: 0.588\n",
            "Step 1700. time since epoch: 949.467. Train acc: 0.807. Train Loss: 0.586\n",
            "Step 1710. time since epoch: 955.106. Train acc: 0.808. Train Loss: 0.585\n",
            "Step 1720. time since epoch: 960.636. Train acc: 0.808. Train Loss: 0.584\n",
            "Step 1730. time since epoch: 966.277. Train acc: 0.808. Train Loss: 0.583\n",
            "Step 1740. time since epoch: 971.831. Train acc: 0.809. Train Loss: 0.582\n",
            "Step 1750. time since epoch: 977.375. Train acc: 0.809. Train Loss: 0.581\n",
            "Step 1760. time since epoch: 982.934. Train acc: 0.809. Train Loss: 0.580\n",
            "--------------------\n",
            "epoch 1, loss 0.5794, train acc 0.809, test acc 0.857, time 1024.0 sec\n"
          ]
        }
      ]
    }
  ]
}