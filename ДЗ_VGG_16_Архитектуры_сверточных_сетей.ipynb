{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOA6N3dzNc2MCLtNaPMW5fs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OVP2023/NN/blob/main/%D0%94%D0%97_VGG_16_%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D1%8B_%D1%81%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D1%85_%D1%81%D0%B5%D1%82%D0%B5%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IhbIEYS1xuTl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision as tv\n",
        "from torchsummary import summary\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6WxSsnzH17Lz",
        "outputId": "71bad1b9-6bae-4202-d888-851d58105791"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0, 0\n",
        "    net.eval()\n",
        "    for X, y in data_iter:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
        "        n += y.shape[0]\n",
        "    return acc_sum.item() / n\n",
        "\n",
        "def train(net, train_iter, test_iter, trainer, num_epochs):\n",
        "    net.to(device)\n",
        "    loss = nn.CrossEntropyLoss(reduction='sum')\n",
        "    net.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "\n",
        "        for i, (X, y) in enumerate(train_iter):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            trainer.zero_grad()\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y)\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.shape[0]\n",
        "\n",
        "            if i % 10 == 0:\n",
        "              print(f\"Step {i}. time since epoch: {time.time() -  start:.3f}. \"\n",
        "                    f\"Train acc: {train_acc_sum / n:.3f}. Train Loss: {train_l_sum / n:.3f}\")\n",
        "            torch.cuda.empty_cache()\n",
        "        test_acc = evaluate_accuracy(test_iter, net.to(device))\n",
        "        print('-' * 20)\n",
        "        print(f'epoch {epoch + 1}, loss {train_l_sum / n:.4f}, train acc {train_acc_sum / n:.3f}'\n",
        "              f', test acc {test_acc:.3f}, time {time.time() - start:.1f} sec')"
      ],
      "metadata": {
        "id": "AhmpQeyrz60Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transoforms = tv.transforms.Compose([tv.transforms.Grayscale(3), tv.transforms.Resize((224, 224)), tv.transforms.ToTensor()])\n",
        "Batch_size = 64\n",
        "train_dataset = tv.datasets.EMNIST('.', split='balanced', train=True, download=True, transform=transoforms)\n",
        "test_dataset = tv.datasets.EMNIST('.', split='balanced', train=False, download=True, transform=transoforms)\n",
        "\n",
        "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=Batch_size, shuffle=True)\n",
        "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=Batch_size)"
      ],
      "metadata": {
        "id": "p7ctWT5dz68s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXvEuDt9-3pv",
        "outputId": "35a9111e-83b9-4e77-ed0a-43f490bfac02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset EMNIST\n",
              "    Number of datapoints: 112800\n",
              "    Root location: .\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Grayscale(num_output_channels=3)\n",
              "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset.classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBsbFbYi_UK7",
        "outputId": "89f1a765-5fff-480f-a6b5-78ca5e044104"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg16 = tv.models.vgg16(weights=tv.models.VGG16_Weights.IMAGENET1K_V1)\n",
        "\n",
        "for param in model_vgg16.parameters():\n",
        "    param.requires_glad = False\n",
        "\n",
        "classifier_list = list(model_vgg16.classifier.children())\n",
        "classifier_list.pop()\n",
        "num_classes = 47\n",
        "classifier_list.append(nn.Linear(4096,num_classes))\n",
        "model_vgg16.classifier = nn.Sequential(*classifier_list)\n",
        "\n",
        "optimizer_vgg16 = torch.optim.Adam(model_vgg16.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "NnFlzWEGEljP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model_vgg16.to(device), input_size=(3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_j2h4gw1B41",
        "outputId": "3e634d49-8b62-4377-d2bc-44bed8f68db1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-2         [-1, 64, 224, 224]               0\n",
            "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
            "              ReLU-4         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
            "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
            "              ReLU-7        [-1, 128, 112, 112]               0\n",
            "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
            "              ReLU-9        [-1, 128, 112, 112]               0\n",
            "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
            "             ReLU-12          [-1, 256, 56, 56]               0\n",
            "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-14          [-1, 256, 56, 56]               0\n",
            "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-16          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
            "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-19          [-1, 512, 28, 28]               0\n",
            "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-21          [-1, 512, 28, 28]               0\n",
            "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-23          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
            "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-26          [-1, 512, 14, 14]               0\n",
            "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-28          [-1, 512, 14, 14]               0\n",
            "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-30          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
            "           Linear-33                 [-1, 4096]     102,764,544\n",
            "             ReLU-34                 [-1, 4096]               0\n",
            "          Dropout-35                 [-1, 4096]               0\n",
            "           Linear-36                 [-1, 4096]      16,781,312\n",
            "             ReLU-37                 [-1, 4096]               0\n",
            "          Dropout-38                 [-1, 4096]               0\n",
            "           Linear-39                   [-1, 47]         192,559\n",
            "================================================================\n",
            "Total params: 134,453,103\n",
            "Trainable params: 134,453,103\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 218.77\n",
            "Params size (MB): 512.90\n",
            "Estimated Total Size (MB): 732.25\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_vgg16, train_data, test_data, optimizer_vgg16,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQhq4UqgKtHd",
        "outputId": "8e69440b-8ac1-48c1-8bdd-c0febea47a41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0. time since epoch: 1.323. Train acc: 0.031. Train Loss: 3.898\n",
            "Step 10. time since epoch: 10.830. Train acc: 0.026. Train Loss: 3.984\n",
            "Step 20. time since epoch: 20.178. Train acc: 0.024. Train Loss: 3.928\n",
            "Step 30. time since epoch: 29.548. Train acc: 0.024. Train Loss: 3.909\n",
            "Step 40. time since epoch: 38.899. Train acc: 0.021. Train Loss: 3.897\n",
            "Step 50. time since epoch: 48.342. Train acc: 0.020. Train Loss: 3.889\n",
            "Step 60. time since epoch: 57.808. Train acc: 0.019. Train Loss: 3.884\n",
            "Step 70. time since epoch: 67.284. Train acc: 0.019. Train Loss: 3.880\n",
            "Step 80. time since epoch: 76.813. Train acc: 0.019. Train Loss: 3.878\n",
            "Step 90. time since epoch: 86.287. Train acc: 0.020. Train Loss: 3.875\n",
            "Step 100. time since epoch: 95.842. Train acc: 0.021. Train Loss: 3.874\n",
            "Step 110. time since epoch: 105.465. Train acc: 0.021. Train Loss: 3.873\n",
            "Step 120. time since epoch: 114.961. Train acc: 0.020. Train Loss: 3.871\n",
            "Step 130. time since epoch: 124.449. Train acc: 0.020. Train Loss: 3.870\n",
            "Step 140. time since epoch: 133.878. Train acc: 0.020. Train Loss: 3.869\n",
            "Step 150. time since epoch: 143.390. Train acc: 0.020. Train Loss: 3.868\n",
            "Step 160. time since epoch: 152.903. Train acc: 0.020. Train Loss: 3.867\n",
            "Step 170. time since epoch: 162.494. Train acc: 0.022. Train Loss: 3.860\n",
            "Step 180. time since epoch: 172.033. Train acc: 0.023. Train Loss: 3.859\n",
            "Step 190. time since epoch: 181.593. Train acc: 0.025. Train Loss: 3.853\n",
            "Step 200. time since epoch: 191.175. Train acc: 0.031. Train Loss: 3.826\n",
            "Step 210. time since epoch: 200.735. Train acc: 0.039. Train Loss: 3.779\n",
            "Step 220. time since epoch: 210.265. Train acc: 0.052. Train Loss: 3.715\n",
            "Step 230. time since epoch: 219.779. Train acc: 0.065. Train Loss: 3.650\n",
            "Step 240. time since epoch: 229.338. Train acc: 0.081. Train Loss: 3.578\n",
            "Step 250. time since epoch: 238.943. Train acc: 0.098. Train Loss: 3.501\n",
            "Step 260. time since epoch: 248.524. Train acc: 0.113. Train Loss: 3.430\n",
            "Step 270. time since epoch: 258.070. Train acc: 0.128. Train Loss: 3.356\n",
            "Step 280. time since epoch: 267.591. Train acc: 0.146. Train Loss: 3.281\n",
            "Step 290. time since epoch: 277.082. Train acc: 0.161. Train Loss: 3.214\n",
            "Step 300. time since epoch: 286.615. Train acc: 0.178. Train Loss: 3.145\n",
            "Step 310. time since epoch: 296.143. Train acc: 0.194. Train Loss: 3.079\n",
            "Step 320. time since epoch: 305.680. Train acc: 0.207. Train Loss: 3.021\n",
            "Step 330. time since epoch: 315.159. Train acc: 0.220. Train Loss: 2.963\n",
            "Step 340. time since epoch: 324.698. Train acc: 0.234. Train Loss: 2.906\n",
            "Step 350. time since epoch: 334.232. Train acc: 0.247. Train Loss: 2.851\n",
            "Step 360. time since epoch: 343.762. Train acc: 0.260. Train Loss: 2.797\n",
            "Step 370. time since epoch: 353.300. Train acc: 0.272. Train Loss: 2.744\n",
            "Step 380. time since epoch: 362.783. Train acc: 0.284. Train Loss: 2.695\n",
            "Step 390. time since epoch: 372.311. Train acc: 0.296. Train Loss: 2.647\n",
            "Step 400. time since epoch: 381.887. Train acc: 0.306. Train Loss: 2.604\n",
            "Step 410. time since epoch: 391.420. Train acc: 0.317. Train Loss: 2.561\n",
            "Step 420. time since epoch: 400.918. Train acc: 0.326. Train Loss: 2.522\n",
            "Step 430. time since epoch: 410.347. Train acc: 0.337. Train Loss: 2.481\n",
            "Step 440. time since epoch: 419.852. Train acc: 0.346. Train Loss: 2.442\n",
            "Step 450. time since epoch: 429.348. Train acc: 0.355. Train Loss: 2.406\n",
            "Step 460. time since epoch: 438.855. Train acc: 0.363. Train Loss: 2.372\n",
            "Step 470. time since epoch: 448.362. Train acc: 0.372. Train Loss: 2.337\n",
            "Step 480. time since epoch: 457.865. Train acc: 0.380. Train Loss: 2.304\n",
            "Step 490. time since epoch: 467.380. Train acc: 0.388. Train Loss: 2.274\n",
            "Step 500. time since epoch: 476.881. Train acc: 0.396. Train Loss: 2.242\n",
            "Step 510. time since epoch: 486.360. Train acc: 0.403. Train Loss: 2.213\n",
            "Step 520. time since epoch: 495.813. Train acc: 0.410. Train Loss: 2.184\n",
            "Step 530. time since epoch: 505.284. Train acc: 0.417. Train Loss: 2.156\n",
            "Step 540. time since epoch: 514.814. Train acc: 0.423. Train Loss: 2.128\n",
            "Step 550. time since epoch: 524.325. Train acc: 0.429. Train Loss: 2.104\n",
            "Step 560. time since epoch: 533.849. Train acc: 0.435. Train Loss: 2.080\n",
            "Step 570. time since epoch: 543.319. Train acc: 0.442. Train Loss: 2.055\n",
            "Step 580. time since epoch: 552.877. Train acc: 0.447. Train Loss: 2.032\n",
            "Step 590. time since epoch: 562.469. Train acc: 0.453. Train Loss: 2.009\n",
            "Step 600. time since epoch: 572.024. Train acc: 0.459. Train Loss: 1.987\n",
            "Step 610. time since epoch: 581.580. Train acc: 0.464. Train Loss: 1.966\n",
            "Step 620. time since epoch: 591.035. Train acc: 0.469. Train Loss: 1.946\n",
            "Step 630. time since epoch: 600.564. Train acc: 0.473. Train Loss: 1.927\n",
            "Step 640. time since epoch: 610.079. Train acc: 0.478. Train Loss: 1.908\n",
            "Step 650. time since epoch: 619.575. Train acc: 0.483. Train Loss: 1.888\n",
            "Step 660. time since epoch: 629.073. Train acc: 0.487. Train Loss: 1.869\n",
            "Step 670. time since epoch: 638.536. Train acc: 0.492. Train Loss: 1.849\n",
            "Step 680. time since epoch: 648.128. Train acc: 0.497. Train Loss: 1.833\n",
            "Step 690. time since epoch: 657.644. Train acc: 0.501. Train Loss: 1.816\n",
            "Step 700. time since epoch: 667.164. Train acc: 0.504. Train Loss: 1.800\n",
            "Step 710. time since epoch: 676.660. Train acc: 0.508. Train Loss: 1.784\n",
            "Step 720. time since epoch: 686.077. Train acc: 0.512. Train Loss: 1.767\n",
            "Step 730. time since epoch: 695.583. Train acc: 0.516. Train Loss: 1.752\n",
            "Step 740. time since epoch: 705.095. Train acc: 0.519. Train Loss: 1.736\n",
            "Step 750. time since epoch: 714.630. Train acc: 0.523. Train Loss: 1.721\n",
            "Step 760. time since epoch: 724.159. Train acc: 0.527. Train Loss: 1.705\n",
            "Step 770. time since epoch: 733.668. Train acc: 0.531. Train Loss: 1.690\n",
            "Step 780. time since epoch: 743.168. Train acc: 0.534. Train Loss: 1.677\n",
            "Step 790. time since epoch: 752.704. Train acc: 0.537. Train Loss: 1.664\n",
            "Step 800. time since epoch: 762.205. Train acc: 0.540. Train Loss: 1.651\n",
            "Step 810. time since epoch: 771.669. Train acc: 0.544. Train Loss: 1.638\n",
            "Step 820. time since epoch: 781.144. Train acc: 0.547. Train Loss: 1.626\n",
            "Step 830. time since epoch: 790.676. Train acc: 0.550. Train Loss: 1.613\n",
            "Step 840. time since epoch: 800.206. Train acc: 0.553. Train Loss: 1.601\n",
            "Step 850. time since epoch: 809.731. Train acc: 0.556. Train Loss: 1.589\n",
            "Step 860. time since epoch: 819.291. Train acc: 0.559. Train Loss: 1.576\n",
            "Step 870. time since epoch: 828.758. Train acc: 0.562. Train Loss: 1.565\n",
            "Step 880. time since epoch: 838.312. Train acc: 0.565. Train Loss: 1.554\n",
            "Step 890. time since epoch: 847.826. Train acc: 0.567. Train Loss: 1.543\n",
            "Step 900. time since epoch: 857.353. Train acc: 0.570. Train Loss: 1.532\n",
            "Step 910. time since epoch: 866.778. Train acc: 0.572. Train Loss: 1.522\n",
            "Step 920. time since epoch: 876.271. Train acc: 0.575. Train Loss: 1.512\n",
            "Step 930. time since epoch: 885.762. Train acc: 0.577. Train Loss: 1.503\n",
            "Step 940. time since epoch: 895.254. Train acc: 0.580. Train Loss: 1.493\n",
            "Step 950. time since epoch: 904.754. Train acc: 0.583. Train Loss: 1.483\n",
            "Step 960. time since epoch: 914.164. Train acc: 0.585. Train Loss: 1.473\n",
            "Step 970. time since epoch: 923.725. Train acc: 0.588. Train Loss: 1.463\n",
            "Step 980. time since epoch: 933.208. Train acc: 0.590. Train Loss: 1.454\n",
            "Step 990. time since epoch: 942.705. Train acc: 0.592. Train Loss: 1.445\n",
            "Step 1000. time since epoch: 952.182. Train acc: 0.594. Train Loss: 1.436\n",
            "Step 1010. time since epoch: 961.649. Train acc: 0.596. Train Loss: 1.427\n",
            "Step 1020. time since epoch: 971.146. Train acc: 0.598. Train Loss: 1.420\n",
            "Step 1030. time since epoch: 980.659. Train acc: 0.600. Train Loss: 1.412\n",
            "Step 1040. time since epoch: 990.141. Train acc: 0.602. Train Loss: 1.403\n",
            "Step 1050. time since epoch: 999.585. Train acc: 0.604. Train Loss: 1.395\n",
            "Step 1060. time since epoch: 1009.049. Train acc: 0.606. Train Loss: 1.387\n",
            "Step 1070. time since epoch: 1018.536. Train acc: 0.608. Train Loss: 1.379\n",
            "Step 1080. time since epoch: 1028.031. Train acc: 0.610. Train Loss: 1.372\n",
            "Step 1090. time since epoch: 1037.528. Train acc: 0.612. Train Loss: 1.365\n",
            "Step 1100. time since epoch: 1046.957. Train acc: 0.614. Train Loss: 1.357\n",
            "Step 1110. time since epoch: 1056.481. Train acc: 0.616. Train Loss: 1.350\n",
            "Step 1120. time since epoch: 1065.956. Train acc: 0.618. Train Loss: 1.342\n",
            "Step 1130. time since epoch: 1075.446. Train acc: 0.620. Train Loss: 1.334\n",
            "Step 1140. time since epoch: 1084.921. Train acc: 0.622. Train Loss: 1.327\n",
            "Step 1150. time since epoch: 1094.313. Train acc: 0.623. Train Loss: 1.320\n",
            "Step 1160. time since epoch: 1103.776. Train acc: 0.625. Train Loss: 1.314\n",
            "Step 1170. time since epoch: 1113.283. Train acc: 0.627. Train Loss: 1.307\n",
            "Step 1180. time since epoch: 1122.806. Train acc: 0.628. Train Loss: 1.300\n",
            "Step 1190. time since epoch: 1132.266. Train acc: 0.630. Train Loss: 1.294\n",
            "Step 1200. time since epoch: 1141.721. Train acc: 0.632. Train Loss: 1.287\n",
            "Step 1210. time since epoch: 1151.197. Train acc: 0.633. Train Loss: 1.281\n",
            "Step 1220. time since epoch: 1160.671. Train acc: 0.635. Train Loss: 1.275\n",
            "Step 1230. time since epoch: 1170.141. Train acc: 0.636. Train Loss: 1.268\n",
            "Step 1240. time since epoch: 1179.540. Train acc: 0.638. Train Loss: 1.263\n",
            "Step 1250. time since epoch: 1189.037. Train acc: 0.640. Train Loss: 1.256\n",
            "Step 1260. time since epoch: 1198.647. Train acc: 0.641. Train Loss: 1.250\n",
            "Step 1270. time since epoch: 1208.142. Train acc: 0.643. Train Loss: 1.244\n",
            "Step 1280. time since epoch: 1217.599. Train acc: 0.644. Train Loss: 1.238\n",
            "Step 1290. time since epoch: 1227.055. Train acc: 0.646. Train Loss: 1.232\n",
            "Step 1300. time since epoch: 1236.528. Train acc: 0.647. Train Loss: 1.227\n",
            "Step 1310. time since epoch: 1246.001. Train acc: 0.648. Train Loss: 1.221\n",
            "Step 1320. time since epoch: 1255.483. Train acc: 0.650. Train Loss: 1.215\n",
            "Step 1330. time since epoch: 1264.889. Train acc: 0.651. Train Loss: 1.210\n",
            "Step 1340. time since epoch: 1274.395. Train acc: 0.653. Train Loss: 1.204\n",
            "Step 1350. time since epoch: 1283.887. Train acc: 0.654. Train Loss: 1.198\n",
            "Step 1360. time since epoch: 1293.371. Train acc: 0.655. Train Loss: 1.194\n",
            "Step 1370. time since epoch: 1302.860. Train acc: 0.657. Train Loss: 1.189\n",
            "Step 1380. time since epoch: 1312.286. Train acc: 0.658. Train Loss: 1.184\n",
            "Step 1390. time since epoch: 1321.784. Train acc: 0.659. Train Loss: 1.179\n",
            "Step 1400. time since epoch: 1331.288. Train acc: 0.660. Train Loss: 1.174\n",
            "Step 1410. time since epoch: 1340.807. Train acc: 0.661. Train Loss: 1.170\n",
            "Step 1420. time since epoch: 1350.257. Train acc: 0.663. Train Loss: 1.165\n",
            "Step 1430. time since epoch: 1359.670. Train acc: 0.664. Train Loss: 1.160\n",
            "Step 1440. time since epoch: 1369.134. Train acc: 0.665. Train Loss: 1.156\n",
            "Step 1450. time since epoch: 1378.626. Train acc: 0.666. Train Loss: 1.151\n",
            "Step 1460. time since epoch: 1388.106. Train acc: 0.668. Train Loss: 1.146\n",
            "Step 1470. time since epoch: 1397.537. Train acc: 0.669. Train Loss: 1.142\n",
            "Step 1480. time since epoch: 1406.984. Train acc: 0.670. Train Loss: 1.138\n",
            "Step 1490. time since epoch: 1416.459. Train acc: 0.671. Train Loss: 1.134\n",
            "Step 1500. time since epoch: 1425.930. Train acc: 0.672. Train Loss: 1.130\n",
            "Step 1510. time since epoch: 1435.389. Train acc: 0.673. Train Loss: 1.126\n",
            "Step 1520. time since epoch: 1444.787. Train acc: 0.674. Train Loss: 1.122\n",
            "Step 1530. time since epoch: 1454.255. Train acc: 0.675. Train Loss: 1.117\n",
            "Step 1540. time since epoch: 1463.828. Train acc: 0.676. Train Loss: 1.113\n",
            "Step 1550. time since epoch: 1473.294. Train acc: 0.677. Train Loss: 1.109\n",
            "Step 1560. time since epoch: 1482.780. Train acc: 0.678. Train Loss: 1.105\n",
            "Step 1570. time since epoch: 1492.189. Train acc: 0.679. Train Loss: 1.102\n",
            "Step 1580. time since epoch: 1501.668. Train acc: 0.680. Train Loss: 1.098\n",
            "Step 1590. time since epoch: 1511.144. Train acc: 0.680. Train Loss: 1.094\n",
            "Step 1600. time since epoch: 1520.623. Train acc: 0.681. Train Loss: 1.090\n",
            "Step 1610. time since epoch: 1530.047. Train acc: 0.682. Train Loss: 1.087\n",
            "Step 1620. time since epoch: 1539.471. Train acc: 0.683. Train Loss: 1.083\n",
            "Step 1630. time since epoch: 1548.921. Train acc: 0.684. Train Loss: 1.079\n",
            "Step 1640. time since epoch: 1558.384. Train acc: 0.685. Train Loss: 1.076\n",
            "Step 1650. time since epoch: 1567.863. Train acc: 0.686. Train Loss: 1.072\n",
            "Step 1660. time since epoch: 1577.286. Train acc: 0.687. Train Loss: 1.069\n",
            "Step 1670. time since epoch: 1586.712. Train acc: 0.688. Train Loss: 1.065\n",
            "Step 1680. time since epoch: 1596.197. Train acc: 0.688. Train Loss: 1.062\n",
            "Step 1690. time since epoch: 1605.684. Train acc: 0.689. Train Loss: 1.059\n",
            "Step 1700. time since epoch: 1615.184. Train acc: 0.690. Train Loss: 1.055\n",
            "Step 1710. time since epoch: 1624.596. Train acc: 0.691. Train Loss: 1.051\n",
            "Step 1720. time since epoch: 1634.055. Train acc: 0.692. Train Loss: 1.048\n",
            "Step 1730. time since epoch: 1643.532. Train acc: 0.693. Train Loss: 1.045\n",
            "Step 1740. time since epoch: 1652.996. Train acc: 0.693. Train Loss: 1.042\n",
            "Step 1750. time since epoch: 1662.455. Train acc: 0.694. Train Loss: 1.038\n",
            "Step 1760. time since epoch: 1671.840. Train acc: 0.695. Train Loss: 1.035\n",
            "--------------------\n",
            "epoch 1, loss 1.0349, train acc 0.695, test acc 0.852, time 1761.6 sec\n"
          ]
        }
      ]
    }
  ]
}